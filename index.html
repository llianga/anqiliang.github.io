<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peter Tong</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peter Tong</name>
              </p>
              <p>Hi, I am Peter Tong, also go by the name Shengbang Tong(Á´•ÊôüÈÇ¶). I am an incoming PhD student in NYU Courant CS advised by Professor Yann LeCun and Professor Saining Xie. I recently graduateed from UC Berkeley with a triple major in Computer Science, Applied Mathematics(Honor) and Statistic(Honor). I am from Nanjing, China and Melbourne, Australia.
              </p>
              <p style="text-align:center">
                <a href="tsb@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/Peter_Tong_Resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="data/PeterTong-classes.txt">Classes</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=hYlbtl8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/tsb0601">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PeterTong.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PeterTong.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I am currently a undergraduate researcher in Berkeley Artificial Intelligence Lab(BAIR) advised by Prof. Yi Ma. I also work closely with Dr. Yubei Chen from Professor Yann LeCun's lab in NYU and Xili Dai from Professor Harry Shum's lab in HKUST(GZ). I am interested in unsupervised/self-supervised learning, generative models, sparse coding and continual learning. During the Summer, I am extremely fortunate to work in New York with Yubei Chen from Professor Yann Lecun's lab on representation learning. I am also working with Erik Jones from Professor Jacob Steinhardt's group on auditting failures in text-to-image models.
              </p>
            </td>
          </tr>
        </tbody></table>


      <!-- News section -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li><strong>2023-05:</strong> I graduated from UC Berkeley with triple degree Applied Math (Honor), Statistics (Honor) and Computer Science (No honor, because I didn't want to take 16b and 61c too early, but I published quite some interesting work so yay)!!!</li>
              <li><strong>2023-04:</strong> I will be a CS PhD student in NYU Courant advised by <a href="https://yann.lecun.com/" target="_blank">Professor Yann LeCun</a> and <a href="https://sainingxie.com/" target="_blank">Professor Saining Xie</a>. Looking forward to working with Yann and Saining in New York!</li>
              <li><strong>2023-01:</strong> Our paper i-CTRL was accepted at ICLR 2023!</li>
            </ul>
          </td>
        </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications & Preprints (* means equal contribution)</heading>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t10ssl.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2304.03977" id="EMP-SSL">
                <papertitle> EMP-SSL: Towards Self-Supervised Learning in One Epoch </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://yubeichen.com/">Yubei Chen*</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>, <a href = "http://yann.lecun.com/">Yann Lecun</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>Inspired by the newly proposed principle, our work proposes a minimalist method for self-supervised learning that tremendously reduces the epochs which SSL methods take to converge.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uCTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.16782" id="uctrl">
                <papertitle>Unsupervised Learning of Structured Representation via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <a href = "https://yubeichen.com/">Yubei Chen</a>, <a>Mingyang Li</a>, <a>Zengyi Li</a>,  <a>Brent Yi</a>, <a href = "http://yann.lecun.com/">Yann Lecun</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a structured representation that may serve both discriminative and generative purpose</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CSC_CTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2302.09347" id="umlc">
                <papertitle> Closed-Loop Transcription Via Convolutional Sparse Coding</papertitle>
              </a>
              <br>
              <a>Xili Dai</a>, <a>Ke Chen</a>, <strong>Shengbang Tong</strong>, <a>Jingyuan Zhang</a>, <a>Xingjian Gao</a>, <a>Mingyang Li</a>, <a>Druv Pai</a>, <a>Yuexiang Zhai</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a>Lionel M.Ni</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Presented at 2nd SLOWDNN Workshop</em>
              <br>
              <p></p>
              <p>This paper explores the natural inverse in Covolutional Sparse Coding neural network and its application in generative models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MCRSE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2301.01805" id="umlc">
                <papertitle>Unsupervised Manifold Linearizing and Clustering</papertitle>
              </a>
              <br>
              <a href = "https://tianjiaoding.com/">Tianjiao Ding</a>, <strong>Shengbang Tong</strong>, <a>Kwan Ho Ryan Chan</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>,<a>Benjamin David Haeffele</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a represenation and cluster for real life dataset such as CIFAR-10, CIFAR100 and Tiny-ImageNet-200.</p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sdnet.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.12945" id="revisit">
                <papertitle>Revisiting Sparse Convolutional Model for Visual Recognition</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>,  <a>Mingyang Li*</a>, <a>Pengyuan Zhai</a>,  <strong>Shengbang Tong</strong>, <a>Xingjian Gao</a>, <a>Shaolun Huang</a>, <a>Zhihui Zhu</a>, <a>Chong You</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by Nips 2022</em>
              <br>
              <p></p>
              <p>Our method uses differentiable optimization layers that are defined from convolutional sparse coding as drop-in replacements of standard convolutional layers in conventional deep neural networks. We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100 and ImageNet datasets when compared to conventional neural networks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iLDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2202.05411" id="iLDR">
                <papertitle>Incremental Learning of Structured Memory via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a>Ziyang Wu</a>, <a>Mingyang Li</a>, <a>Brent Yi</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by ICLR 2023</em>
              <br>
              <p></p>
              <p>We propose a minimal computational model for learning a structured memory of multiple object classes in an incremental setting</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.06636" id="LDR">
                <papertitle>Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <strong>Shengbang Tong*</strong>, <a>Mingyang Li*</a>, <a>Ziyang Wu*</a>, <a>Kwan Ho Ryan Chan</a>, <a>Pengyuan Zhai</a>, <a>Yaodong Yu</a>, <a>Michael Psenka</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a href = "http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>
              <br>
              <em>Accepted by Entropy Journal</em>
              <br>
              <p></p>
              <p>We propose a new computational framework for learning an explicit generative model for real-world dataset.</p>
            </td>
          </tr>


       <table class="sub-table" style="width: 200px;height: 100px;" align="center">
         <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=y35-AqSkeLIkce_C13W-97DGULFZQWj5YJB3rNARabY&cl=ffffff&w=a"></script>
      </table>
        </tbody></table>




      </td>
    </tr>
  </table>
</body>

</html>
